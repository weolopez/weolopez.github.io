<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Microphone Beamforming Simulator</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6; /* Tailwind gray-100 */
        }
        /* Custom styles for microphone visualization */
        .mic-icon {
            width: 40px;
            height: 40px;
            background-color: #60a5fa; /* Tailwind blue-400 */
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            position: absolute; /* Positioned within the canvas */
            z-index: 10; /* Ensure mics are clickable above beam */
        }
        .mic-icon.selected {
            background-color: #2563eb; /* Tailwind blue-600 */
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }
        .mic-icon.disabled {
            background-color: #9ca3af; /* Tailwind gray-400 */
            cursor: not-allowed;
        }
         .mic-icon.monitoring {
            outline: 3px solid #facc15; /* Tailwind yellow-400 */
            outline-offset: 2px;
        }
        /* Beam visualization */
        #beamCanvas {
            background-color: #e5e7eb; /* Tailwind gray-200 */
            border-radius: 0.5rem; /* Tailwind rounded-lg */
            position: relative;
            overflow: hidden;
            min-height: 300px; /* Ensure canvas has height */
            box-shadow: inset 0 2px 4px rgba(0,0,0,0.06);
        }
        .beam-shape {
            position: absolute;
            bottom: 0;
            left: 50%;
            width: 0;
            height: 0;
            border-left: 50px solid transparent;
            border-right: 50px solid transparent;
            border-bottom: 150px solid rgba(59, 130, 246, 0.5); /* Tailwind blue-500 with opacity */
            transform-origin: 50% 100%;
            transition: transform 0.5s ease-out;
            pointer-events: none; /* Don't interfere with mic clicks */
            z-index: 5; /* Below mic icons */
        }
         /* Message Box Styles */
        #messageBox {
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background-color: #ef4444; /* Tailwind red-500 */
            color: white;
            padding: 0.8rem 1.2rem; /* Slightly smaller padding */
            border-radius: 0.5rem; /* Tailwind rounded-lg */
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            z-index: 1000;
            display: none; /* Hidden by default */
            font-weight: 500;
            font-size: 0.9rem; /* Slightly smaller font */
        }
        #messageBox.success {
             background-color: #22c55e; /* Tailwind green-500 */
        }
        /* Loudness Meter Styles */
        .loudness-meter {
            width: 80px; /* Fixed width for the meter */
            height: 10px;
            appearance: none; /* Remove default appearance */
            border: 1px solid #d1d5db; /* Tailwind gray-300 */
            border-radius: 5px;
            overflow: hidden; /* Ensure progress stays within bounds */
            background-color: #e5e7eb; /* Tailwind gray-200 */
        }
        .loudness-meter::-webkit-progress-bar {
            background-color: #e5e7eb; /* Tailwind gray-200 */
            border-radius: 5px;
        }
        .loudness-meter::-webkit-progress-value {
            background-color: #34d399; /* Tailwind emerald-400 */
            border-radius: 5px;
            transition: width 0.1s linear; /* Smooth transition */
        }
        .loudness-meter::-moz-progress-bar { /* Firefox */
             background-color: #34d399; /* Tailwind emerald-400 */
             border-radius: 5px;
             transition: width 0.1s linear; /* Smooth transition */
        }
        .mic-list-item {
             display: grid;
             grid-template-columns: 1fr auto auto auto; /* Label, Meter, Monitor Btn, Select Btn */
             gap: 0.75rem; /* Tailwind gap-3 */
             align-items: center;
             padding: 0.5rem; /* Tailwind p-2 */
             border: 1px solid #e5e7eb; /* Tailwind border-gray-200 */
             border-radius: 0.375rem; /* Tailwind rounded-md */
        }
        .mic-label {
            /* Tailwind text-sm text-gray-700 truncate */
            min-width: 100px; /* Ensure label doesn't collapse too much */
        }
         .action-button {
             font-size: 0.75rem; /* Tailwind text-xs */
             font-weight: 700; /* Tailwind font-bold */
             padding: 0.25rem 0.5rem; /* Tailwind py-1 px-2 */
             border-radius: 0.25rem; /* Tailwind rounded */
             transition: background-color 0.15s ease-in-out, color 0.15s ease-in-out;
             white-space: nowrap; /* Prevent button text wrapping */
        }
    </style>
</head>
<body class="p-8">
    <div class="max-w-4xl mx-auto bg-white p-6 rounded-lg shadow-md">
        <h1 class="text-2xl font-bold mb-4 text-center text-gray-800">Microphone Beamforming Simulator</h1>

        <div id="messageBox"></div>

        <div class="mb-6">
            <h2 class="text-lg font-semibold mb-3 text-gray-700">1. Available Microphones</h2>
            <p class="text-sm text-gray-500 mb-3">Click 'Detect Microphones'. Then use the 'Monitor' button and the loudness meter to identify your physical mics. Select mics for the beam simulation using the 'Select' button.</p>
            <button id="detectButton" class="bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded-md transition duration-150 ease-in-out shadow">
                Detect Microphones
            </button>
            <div id="micList" class="mt-4 space-y-2">
                </div>
        </div>

        <div class="mb-6">
            <h2 class="text-lg font-semibold mb-3 text-gray-700">2. Configure Array</h2>
            <p class="text-sm text-gray-500 mb-1">Selected for Beamforming: <span id="selectedCount" class="font-medium">0</span></p>
             <p class="text-sm text-gray-500 mb-3">Click on the visualization below to set the desired sound direction for the selected mics.</p>
        </div>

        <div>
            <h2 class="text-lg font-semibold mb-3 text-gray-700">3. Beamforming Visualization</h2>
             <p class="text-sm text-gray-500 mb-3">The blue shape represents the focused listening area (beam). Mic icons outlined in yellow are currently being monitored for loudness.</p>
            <div id="beamCanvas" class="w-full h-80 relative rounded-lg">
                <div id="beam" class="beam-shape hidden"></div>
            </div>
             <p class="text-sm text-gray-500 mt-2">Simulated Source Angle: <span id="sourceAngle" class="font-medium">N/A</span></p>
        </div>

    </div>

    <script>
        const detectButton = document.getElementById('detectButton');
        const micListContainer = document.getElementById('micList');
        const beamCanvas = document.getElementById('beamCanvas');
        const beamElement = document.getElementById('beam');
        const selectedCountSpan = document.getElementById('selectedCount');
        const sourceAngleSpan = document.getElementById('sourceAngle');
        const messageBox = document.getElementById('messageBox');

        let microphones = []; // Store detected microphones { id, label, element, selected, position, visualElement, monitoringInfo }
        let micVisuals = []; // Store visual elements on the canvas
        let canvasCenterX = 0;
        let canvasCenterY = 0;
        let currentlyMonitoredMicIndex = null; // Index of the mic being monitored for loudness
        let lastTargetPosition = { x: 0, y: 0 }; // Store the last target position for beam

        // Structure for monitoring info attached to each mic object
        // monitoringInfo: { stream: null, audioContext: null, analyser: null, animationFrameId: null, loudnessMeterElement: null, monitorButtonElement: null }

        // --- Audio Context (reuse if possible, though creating per stream is safer) ---
        // Note: Creating a new AudioContext per stream is generally more robust,
        // especially across different browsers and scenarios.
        // let sharedAudioContext = null; // Potential optimization, but can be tricky

        // --- Message Display ---
        function showMessage(text, type = 'error', duration = 4000) {
            messageBox.textContent = text;
            // Ensure messageBox class is reset before adding success if needed
            messageBox.className = 'fixed top-5 left-1/2 transform -translate-x-1/2 text-white px-5 py-3 rounded-lg shadow-lg z-[1000] text-sm font-medium'; // Base classes
            if (type === 'success') {
                 messageBox.classList.add('bg-green-500'); // Tailwind green-500
            } else {
                 messageBox.classList.add('bg-red-500'); // Tailwind red-500
            }
            messageBox.style.display = 'block';
            // Clear previous timeouts if any
            if (messageBox.timeoutId) {
                clearTimeout(messageBox.timeoutId);
            }
            messageBox.timeoutId = setTimeout(() => {
                messageBox.style.display = 'none';
                messageBox.timeoutId = null;
            }, duration);
        }

        // --- Microphone Detection ---
        async function getMicrophones() {
            micListContainer.innerHTML = '<p class="text-gray-500">Detecting...</p>';
            // Stop any existing monitoring before clearing mics
            if (currentlyMonitoredMicIndex !== null) {
                await stopLoudnessMonitoring(currentlyMonitoredMicIndex); // Ensure cleanup
            }
            currentlyMonitoredMicIndex = null;

            microphones = []; // Reset
            micVisuals.forEach(mic => mic.remove()); // Clear old visuals
            micVisuals = [];
            beamElement.classList.add('hidden'); // Hide beam
            selectedCountSpan.textContent = '0';
            sourceAngleSpan.textContent = 'N/A';
            lastTargetPosition = { x: 0, y: 0 }; // Reset target position


            try {
                // Request permission - must be done first via getUserMedia
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                stream.getTracks().forEach(track => track.stop()); // Stop the permission stream

                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioInputDevices = devices.filter(device => device.kind === 'audioinput');

                micListContainer.innerHTML = ''; // Clear 'Detecting...'

                if (audioInputDevices.length === 0) {
                    micListContainer.innerHTML = '<p class="text-red-500">No microphones detected.</p>';
                    showMessage('No microphones found.', 'error');
                    return;
                }

                showMessage(`Detected ${audioInputDevices.length} microphone(s).`, 'success', 2000);

                audioInputDevices.forEach((device, index) => {
                    const micData = {
                        id: device.deviceId,
                        label: device.label || `Microphone ${index + 1}`,
                        element: null, // Will be created by createMicListItem
                        selected: false,
                        position: { x: 0, y: 0 },
                        visualElement: null,
                        monitoringInfo: { // Initialize monitoring structure
                            stream: null,
                            audioContext: null,
                            analyser: null,
                            animationFrameId: null,
                            loudnessMeterElement: null,
                            monitorButtonElement: null,
                            dataArray: null // For storing analyser data
                        }
                    };
                    // Create the list item which includes the loudness meter and buttons
                    micData.element = createMicListItem(micData, index);
                    microphones.push(micData);
                    micListContainer.appendChild(micData.element);
                });

               positionMicrophones(); // Position visuals on canvas
               loadBeamformingState(); // Load saved state after positioning mics

           } catch (err) {
                console.error("Error accessing media devices.", err);
                micListContainer.innerHTML = `<p class="text-red-500">Error: ${err.name}. Please grant permission.</p>`;
                if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
                     showMessage('Microphone access denied. Please grant permission.', 'error');
                } else {
                    showMessage(`Error detecting microphones: ${err.message}`, 'error');
                }
            }
        }

        // --- UI Creation ---
        function createMicListItem(micData, index) {
            const div = document.createElement('div');
            div.className = 'mic-list-item'; // Use grid layout class

            // Mic Label
            const labelSpan = document.createElement('span');
            labelSpan.className = 'mic-label';
            labelSpan.textContent = micData.label;
            labelSpan.title = micData.label; // Tooltip for long names
            div.appendChild(labelSpan);

            // Loudness Meter
            const meter = document.createElement('progress');
            meter.className = 'loudness-meter';
            meter.max = 100; // Max value for the meter
            meter.value = 0; // Start at 0
            micData.monitoringInfo.loudnessMeterElement = meter; // Store reference
            div.appendChild(meter);

            // Monitor Button
            const monitorButton = document.createElement('button');
            monitorButton.dataset.micIndex = index;
            monitorButton.className = 'monitor-mic-btn action-button bg-yellow-400 hover:bg-yellow-500 text-yellow-900';
            monitorButton.textContent = 'Monitor';
            monitorButton.addEventListener('click', () => handleMonitorButtonClick(index));
            micData.monitoringInfo.monitorButtonElement = monitorButton; // Store reference
            div.appendChild(monitorButton);

            // Select Button
            const selectButton = document.createElement('button');
            selectButton.dataset.micIndex = index;
            selectButton.className = 'select-mic-btn action-button bg-gray-300 hover:bg-gray-400 text-gray-800';
            selectButton.textContent = 'Select';
            selectButton.addEventListener('click', () => toggleMicSelection(index));
            micData.monitoringInfo.selectButtonElement = selectButton; // Store reference for easy access
            div.appendChild(selectButton);

            return div;
        }


        function createMicVisual(index) {
            const mic = microphones[index];
            const visual = document.createElement('div');
            visual.className = 'mic-icon';
            visual.textContent = index + 1;
            visual.style.left = `${mic.position.x}px`;
            visual.style.top = `${mic.position.y}px`;
            visual.title = mic.label;
            visual.addEventListener('click', (e) => {
                 e.stopPropagation();
                 toggleMicSelection(index);
            });
            beamCanvas.appendChild(visual);
            mic.visualElement = visual;
            micVisuals.push(visual);
            return visual;
        }

        // --- Microphone Positioning ---
        function positionMicrophones() {
            const canvasRect = beamCanvas.getBoundingClientRect();
             if (!canvasRect || canvasRect.width === 0) return; // Avoid errors if canvas not ready

            const numMics = microphones.length;
            canvasCenterX = canvasRect.width / 2;
            canvasCenterY = canvasRect.height * 0.75; // Position mics lower down

            microphones.forEach((mic, index) => {
                let posX, posY;
                if (numMics === 1) {
                    posX = canvasCenterX;
                } else {
                    const totalWidth = canvasRect.width * 0.7; // Use 70% of canvas width
                    const spacing = numMics > 1 ? totalWidth / (numMics - 1) : 0;
                    const startX = canvasCenterX - (totalWidth / 2);
                    posX = startX + index * spacing;
                }
                posY = canvasCenterY;

                mic.position.x = posX - 20; // Adjust for icon center
                mic.position.y = posY - 20; // Adjust for icon center

                // Create or update visual element
                if (!mic.visualElement) {
                    createMicVisual(index);
                } else {
                    mic.visualElement.style.left = `${mic.position.x}px`;
                    mic.visualElement.style.top = `${mic.position.y}px`;
                }
            });
        }


        // --- Selection Logic (for Beamforming) ---
        function toggleMicSelection(index) {
            const mic = microphones[index];
            mic.selected = !mic.selected;

            // Update list button appearance (Select Button)
            const button = mic.monitoringInfo.selectButtonElement;
            button.textContent = mic.selected ? 'Deselect' : 'Select';
            button.classList.toggle('bg-green-500', mic.selected);
            button.classList.toggle('hover:bg-green-600', mic.selected);
            button.classList.toggle('text-white', mic.selected);
            button.classList.toggle('bg-gray-300', !mic.selected);
            button.classList.toggle('hover:bg-gray-400', !mic.selected);
            button.classList.toggle('text-gray-800', !mic.selected);

            // Update visual icon appearance
            if (mic.visualElement) {
                mic.visualElement.classList.toggle('selected', mic.selected);
            }
           updateSelectedCount();
           saveBeamformingState(); // Save state after selection change
           updateBeam(lastTargetPosition.x, lastTargetPosition.y); // Update beam based on current selection and saved target
       }

        function updateSelectedCount() {
             const count = microphones.filter(m => m.selected).length;
             selectedCountSpan.textContent = count;
        }

        // --- Loudness Monitoring Logic ---

        async function handleMonitorButtonClick(index) {
            if (currentlyMonitoredMicIndex === index) {
                // If clicking the currently monitored mic, stop monitoring it
                await stopLoudnessMonitoring(index);
            } else {
                // If clicking a different mic, stop the current one (if any) and start the new one
                if (currentlyMonitoredMicIndex !== null) {
                    await stopLoudnessMonitoring(currentlyMonitoredMicIndex);
                }
                await startLoudnessMonitoring(index);
            }
        }

        async function startLoudnessMonitoring(index) {
            const mic = microphones[index];
            if (!mic || mic.monitoringInfo.stream) return; // Already monitoring or mic doesn't exist

            const constraints = {
                audio: { deviceId: { exact: mic.id } },
                video: false
            };

            try {
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                mic.monitoringInfo.stream = stream;

                // Use a new AudioContext for each stream for robustness
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                mic.monitoringInfo.audioContext = audioContext;

                const source = audioContext.createMediaStreamSource(stream);
                const analyser = audioContext.createAnalyser();
                analyser.fftSize = 256; // Smaller FFT size for faster response
                const bufferLength = analyser.frequencyBinCount;
                mic.monitoringInfo.dataArray = new Uint8Array(bufferLength); // Store data array

                source.connect(analyser);
                // Do NOT connect analyser to destination to avoid feedback

                mic.monitoringInfo.analyser = analyser;

                // Update UI
                mic.monitoringInfo.monitorButtonElement.textContent = 'Stop';
                mic.monitoringInfo.monitorButtonElement.classList.remove('bg-yellow-400', 'hover:bg-yellow-500', 'text-yellow-900');
                mic.monitoringInfo.monitorButtonElement.classList.add('bg-red-500', 'hover:bg-red-600', 'text-white');
                 if(mic.visualElement) mic.visualElement.classList.add('monitoring');


                currentlyMonitoredMicIndex = index;

                // Start the animation loop
                const updateLoop = () => {
                    updateLoudness(index);
                    // Check if monitoring is still active before scheduling next frame
                    if (mic.monitoringInfo.analyser) {
                       mic.monitoringInfo.animationFrameId = requestAnimationFrame(updateLoop);
                    }
                };
                mic.monitoringInfo.animationFrameId = requestAnimationFrame(updateLoop);
                 showMessage(`Monitoring ${mic.label}...`, 'success', 2000);

            } catch (err) {
                console.error(`Error starting monitoring for mic ${index}:`, err);
                showMessage(`Could not start monitoring: ${err.name}`, 'error');
                // Clean up partially started resources if error occurred
                await stopLoudnessMonitoring(index); // Attempt cleanup
            }
        }

        async function stopLoudnessMonitoring(index) {
            const mic = microphones[index];
            if (!mic || !mic.monitoringInfo.stream) return; // Not monitoring or mic doesn't exist

            // Cancel animation frame
            if (mic.monitoringInfo.animationFrameId) {
                cancelAnimationFrame(mic.monitoringInfo.animationFrameId);
                mic.monitoringInfo.animationFrameId = null;
            }

            // Stop media stream tracks
            mic.monitoringInfo.stream.getTracks().forEach(track => track.stop());
            mic.monitoringInfo.stream = null;

            // Close AudioContext
            if (mic.monitoringInfo.audioContext && mic.monitoringInfo.audioContext.state !== 'closed') {
               try {
                    await mic.monitoringInfo.audioContext.close();
               } catch (e) {
                   console.warn("Error closing AudioContext (possibly already closed):", e);
               }
            }
            mic.monitoringInfo.audioContext = null;
            mic.monitoringInfo.analyser = null; // Clear analyser reference
            mic.monitoringInfo.dataArray = null; // Clear data array reference


            // Update UI
            mic.monitoringInfo.loudnessMeterElement.value = 0; // Reset meter
            mic.monitoringInfo.monitorButtonElement.textContent = 'Monitor';
            mic.monitoringInfo.monitorButtonElement.classList.remove('bg-red-500', 'hover:bg-red-600', 'text-white');
            mic.monitoringInfo.monitorButtonElement.classList.add('bg-yellow-400', 'hover:bg-yellow-500', 'text-yellow-900');
             if(mic.visualElement) mic.visualElement.classList.remove('monitoring');


            if (currentlyMonitoredMicIndex === index) {
                currentlyMonitoredMicIndex = null;
            }
             showMessage(`Stopped monitoring ${mic.label}.`, 'success', 1500);
        }

        function updateLoudness(index) {
            const mic = microphones[index];
            const analyser = mic.monitoringInfo.analyser;
            const dataArray = mic.monitoringInfo.dataArray;
            const meter = mic.monitoringInfo.loudnessMeterElement;

            if (!analyser || !dataArray || !meter) return; // Exit if resources aren't ready

            analyser.getByteFrequencyData(dataArray); // Fill dataArray with frequency data

            // Calculate an average volume level
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i];
            }
            let average = sum / dataArray.length;

            // Scale the average to the meter range (0-100)
            // The values in dataArray range from 0 to 255.
            // We can map this range non-linearly if needed, but linear is simple.
            const meterValue = (average / 255) * 100 * 1.5; // Multiply by 1.5 to make lower volumes more visible
            meter.value = Math.min(meterValue, 100); // Clamp to max 100
        }


        // --- Beam Visualization Logic ---
        function updateBeam(targetX = canvasCenterX, targetY = 0) {
            const selectedMics = microphones.filter(m => m.selected);

            // Store the target position
            lastTargetPosition = { x: targetX, y: targetY };

            if (selectedMics.length < 2 || canvasCenterX === 0) { // Need at least 2 mics and canvas ready
                beamElement.classList.add('hidden');
                sourceAngleSpan.textContent = 'N/A';
                saveBeamformingState(); // Save state even if beam is hidden
                return;
            }

            let centroidX = 0;
            let centroidY = 0;
            selectedMics.forEach(mic => {
                centroidX += mic.position.x + 20; // Center of icon
                centroidY += mic.position.y + 20; // Center of icon
            });
            centroidX /= selectedMics.length;
            centroidY /= selectedMics.length;

            const deltaX = targetX - centroidX;
            const deltaY = centroidY - targetY; // Y is inverted
            let angleRad = Math.atan2(deltaY, deltaX);
            let angleDeg = 90 - (angleRad * (180 / Math.PI)); // 0 deg is up
             if (angleDeg <= -180) angleDeg += 360; // Normalize to -180 to 180
             if (angleDeg > 180) angleDeg -= 360;

            beamElement.classList.remove('hidden');
            beamElement.style.left = `${centroidX}px`;
            beamElement.style.bottom = `${beamCanvas.offsetHeight - centroidY}px`;
            beamElement.style.transform = `translateX(-50%) rotate(${-angleDeg}deg)`;

            sourceAngleSpan.textContent = `${angleDeg.toFixed(1)}Â°`;

            saveBeamformingState(); // Save state after beam update
        }

        // --- State Saving and Loading ---
        function saveBeamformingState() {
            const state = {
                selectedMicIds: microphones.filter(m => m.selected).map(m => m.id),
                targetPosition: lastTargetPosition,
                monitoredMicIndex: currentlyMonitoredMicIndex // Add monitored mic index
            };
            localStorage.setItem('beamformingState', JSON.stringify(state));
        }

        function loadBeamformingState() {
            const savedState = localStorage.getItem('beamformingState');
            if (!savedState) {
                // Set initial target position if no state is saved
                lastTargetPosition = { x: canvasCenterX, y: 0 };
                return;
            }

            try {
                const state = JSON.parse(savedState);
                if (state && state.selectedMicIds && state.targetPosition !== undefined && state.monitoredMicIndex !== undefined) {
                    // Apply selected mics
                    microphones.forEach(mic => {
                        mic.selected = state.selectedMicIds.includes(mic.id);
                        // Update visual and button state immediately
                        if (mic.visualElement) {
                            mic.visualElement.classList.toggle('selected', mic.selected);
                        }
                        const button = mic.monitoringInfo.selectButtonElement;
                        if (button) {
                            button.textContent = mic.selected ? 'Deselect' : 'Select';
                            button.classList.toggle('bg-green-500', mic.selected);
                            button.classList.toggle('hover:bg-green-600', mic.selected);
                            button.classList.toggle('text-white', mic.selected);
                            button.classList.toggle('bg-gray-300', !mic.selected);
                            button.classList.toggle('hover:bg-gray-400', !mic.selected);
                            button.classList.toggle('text-gray-800', !mic.selected);
                        }
                    });
                    updateSelectedCount(); // Update the count display

                    // Apply target position
                    lastTargetPosition = state.targetPosition;

                    // Apply monitored mic state
                    if (state.monitoredMicIndex !== null && microphones[state.monitoredMicIndex]) {
                         // Need to wait for user interaction to start monitoring due to browser autoplay policies
                         // We can't automatically start the stream here, but we can set the UI state
                         currentlyMonitoredMicIndex = state.monitoredMicIndex;
                         const mic = microphones[currentlyMonitoredMicIndex];
                         if(mic.visualElement) mic.visualElement.classList.add('monitoring');
                         const monitorButton = mic.monitoringInfo.monitorButtonElement;
                         if (monitorButton) {
                             monitorButton.textContent = 'Stop';
                             monitorButton.classList.remove('bg-yellow-400', 'hover:bg-yellow-500', 'text-yellow-900');
                             monitorButton.classList.add('bg-red-500', 'hover:bg-red-600', 'text-white');
                         }
                         showMessage(`Monitoring state loaded for ${mic.label}. Click Stop/Monitor to resume.`, 'success', 3000);

                    } else {
                         currentlyMonitoredMicIndex = null;
                    }


                    // Update beam based on loaded state
                    updateBeam(lastTargetPosition.x, lastTargetPosition.y);
                } else {
                    console.warn("Invalid state loaded from localStorage.");
                    // Set initial target position if state is invalid
                    lastTargetPosition = { x: canvasCenterX, y: 0 };
                    currentlyMonitoredMicIndex = null; // Reset monitored state
                }
            } catch (e) {
                console.error("Error parsing state from localStorage:", e);
                // Set initial target position if parsing fails
                lastTargetPosition = { x: canvasCenterX, y: 0 };
                currentlyMonitoredMicIndex = null; // Reset monitored state
            }
        }

        // --- Event Listeners ---
        detectButton.addEventListener('click', getMicrophones);

        beamCanvas.addEventListener('click', (event) => {
            // Ignore clicks on the mic icons themselves
            if (event.target.classList.contains('mic-icon')) {
                return;
            }
            const rect = beamCanvas.getBoundingClientRect();
            const clickX = event.clientX - rect.left;
            const clickY = event.clientY - rect.top;
            // Use the last calculated angle's target if just updating selection
            // Or use the new click target if clicking the canvas
            updateBeam(clickX, clickY);
        });

        window.addEventListener('resize', () => {
             // Debounce resize event slightly? For now, just reposition.
             if (microphones.length > 0) {
                 positionMicrophones(); // Recalculate positions and update visuals
                 updateBeam(lastTargetPosition.x, lastTargetPosition.y); // Update beam based on new positions/size and saved target
             }
        });

        // Initial state message
        micListContainer.innerHTML = '<p class="text-gray-500">Click "Detect Microphones" to begin.</p>';

    </script>
</body>
</html>
        detectButton.addEventListener('click', getMicrophones);

        beamCanvas.addEventListener('click', (event) => {
            // Ignore clicks on the mic icons themselves
            if (event.target.classList.contains('mic-icon')) {
                return;
            }
            const rect = beamCanvas.getBoundingClientRect();
            const clickX = event.clientX - rect.left;
            const clickY = event.clientY - rect.top;
            // Use the last calculated angle's target if just updating selection
            // Or use the new click target if clicking the canvas
            updateBeam(clickX, clickY);
        });

        window.addEventListener('resize', () => {
             // Debounce resize event slightly? For now, just reposition.
             if (microphones.length > 0) {
                 positionMicrophones(); // Recalculate positions and update visuals
                 updateBeam(); // Update beam based on new positions/size
             }
        });

        // Initial state message
        micListContainer.innerHTML = '<p class="text-gray-500">Click "Detect Microphones" to begin.</p>';

    </script>
</body>
</html>
